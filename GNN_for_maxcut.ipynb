{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# notebooks.GNN_for_maxcut"
      ],
      "metadata": {
        "id": "sO8tmeutoJVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install torch-geometric"
      ],
      "metadata": {
        "id": "yRSf1Bl4oxqd"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data, DataLoader\n",
        "from torch_geometric.nn import GCNConv\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nfYlWbu6oSbT"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(num_samples=1000, graph_size=4, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    datasets = []\n",
        "    labels = []\n",
        "    for _ in range(num_samples):\n",
        "        q = np.random.randint(-5, 6, (graph_size, graph_size))\n",
        "        q_s = (q + q.T) / 2\n",
        "        spins = np.random.choice([-1, 1], size=graph_size)\n",
        "        cut_value = 0\n",
        "        for i in range(graph_size):\n",
        "            for j in range(i + 1, graph_size):\n",
        "                if spins[i] != spins[j]:\n",
        "                    cut_value += abs(q_s[i, j])\n",
        "        input_features = q_s[np.triu_indices(graph_size, k=1)]\n",
        "        datasets.append(input_features)\n",
        "        labels.append(cut_value)\n",
        "    return torch.tensor(datasets, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "H6L-SyKrooK4"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_single_graph(q_s):\n",
        "    graph_size = q_s.shape[0]\n",
        "    edge_index = torch.combinations(torch.arange(graph_size), r=2).t()\n",
        "    edge_weights = q_s[np.triu_indices(graph_size, k=1)]\n",
        "    data = Data(\n",
        "        x=torch.eye(graph_size),\n",
        "        edge_index=edge_index,\n",
        "        edge_attr=torch.tensor(edge_weights, dtype=torch.float32).unsqueeze(1),\n",
        "        y=None,\n",
        "    )\n",
        "    return data"
      ],
      "metadata": {
        "id": "0Lp38XBSpfKN"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GNN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
        "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = torch.mean(x, dim=0)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "73OyCd1woVAw"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(num_samples=1000, graph_size=4):\n",
        "    datasets = []\n",
        "    labels = []\n",
        "    features, target_labels = generate_dataset(num_samples=num_samples, graph_size=graph_size)\n",
        "    for i in range(num_samples):\n",
        "        edge_index = torch.combinations(torch.arange(graph_size), r=2).t()\n",
        "        data = Data(\n",
        "            x=torch.eye(graph_size),\n",
        "            edge_index=edge_index,\n",
        "            edge_attr=features[i].unsqueeze(1),\n",
        "            y=target_labels[i],\n",
        "        )\n",
        "        datasets.append(data)\n",
        "    return datasets"
      ],
      "metadata": {
        "id": "gx94myMvoVdy"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "S42_-L1doG4B"
      },
      "outputs": [],
      "source": [
        "def train_gnn(num_samples=1000, graph_size=4, epochs=50, batch_size=32):\n",
        "    dataset = prepare_dataset(num_samples=num_samples, graph_size=graph_size)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "    model = GNN(input_dim=graph_size, hidden_dim=16, output_dim=1)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch in dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            output = model(batch)\n",
        "            loss = loss_fn(output.squeeze(), batch.y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_gnn(model, num_samples=100, graph_size=4):\n",
        "    test_dataset = prepare_dataset(num_samples=num_samples, graph_size=graph_size)\n",
        "    dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            output = model(batch)\n",
        "            loss = F.mse_loss(output.squeeze(), batch.y)\n",
        "            total_loss += loss.item()\n",
        "    print(f\"Test Loss: {total_loss / len(dataloader):.4f}\")"
      ],
      "metadata": {
        "id": "66jx9hD8of42"
      },
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_single_graph(model, graph):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prediction = model(graph).item()\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "mxAUolw7ppou"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exact_maxcut(q_s):\n",
        "    n = q_s.shape[0]\n",
        "    max_cut = 0\n",
        "    for partition in range(1, 1 << n - 1):\n",
        "        set_a = [i for i in range(n) if (partition & (1 << i))]\n",
        "        set_b = [i for i in range(n) if not (partition & (1 << i))]\n",
        "        cut_value = 0\n",
        "        for i in set_a:\n",
        "            for j in set_b:\n",
        "                cut_value += abs(q_s[i, j])\n",
        "\n",
        "        max_cut = max(max_cut, cut_value)\n",
        "    return max_cut"
      ],
      "metadata": {
        "id": "YuP030DPpw_j"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q_s = np.array([[0, 3, 0, -2],\n",
        "                [3, 0, 4, 0],\n",
        "                [0, 4, 0, 1],\n",
        "                [-2, 0, 1, 0]])\n",
        "\n",
        "test_graph = create_single_graph(q_s)"
      ],
      "metadata": {
        "id": "Na0x02hgpQD7"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_gnn(num_samples=1000, graph_size=4, epochs=100)\n",
        "true_maxcut = exact_maxcut(q_s)\n",
        "predicted_cut = test_single_graph(model, test_graph)\n",
        "approximation_ratio = predicted_cut / true_maxcut if true_maxcut != 0 else 0\n",
        "print(f\"Predicted MaxCut value for the square graph: {predicted_cut:.2f}\")\n",
        "print(f\"True MaxCut value for the square graph: {true_maxcut:.2f}\")\n",
        "print(f\"Approximation Ratio: {approximation_ratio:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9lsrT51pnoD",
        "outputId": "b4096e7f-ccf5-48a5-d396-bfb28f7e66c7"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 827.2873\n",
            "Epoch 2/100, Loss: 330.7068\n",
            "Epoch 3/100, Loss: 321.5484\n",
            "Epoch 4/100, Loss: 315.8487\n",
            "Epoch 5/100, Loss: 318.1853\n",
            "Epoch 6/100, Loss: 322.8162\n",
            "Epoch 7/100, Loss: 316.2698\n",
            "Epoch 8/100, Loss: 323.6266\n",
            "Epoch 9/100, Loss: 325.0360\n",
            "Epoch 10/100, Loss: 320.6282\n",
            "Epoch 11/100, Loss: 320.4912\n",
            "Epoch 12/100, Loss: 319.1008\n",
            "Epoch 13/100, Loss: 323.4767\n",
            "Epoch 14/100, Loss: 319.1054\n",
            "Epoch 15/100, Loss: 321.8447\n",
            "Epoch 16/100, Loss: 327.5805\n",
            "Epoch 17/100, Loss: 321.0409\n",
            "Epoch 18/100, Loss: 324.4548\n",
            "Epoch 19/100, Loss: 318.8719\n",
            "Epoch 20/100, Loss: 329.6692\n",
            "Epoch 21/100, Loss: 322.8546\n",
            "Epoch 22/100, Loss: 316.7159\n",
            "Epoch 23/100, Loss: 323.1354\n",
            "Epoch 24/100, Loss: 320.2368\n",
            "Epoch 25/100, Loss: 325.6627\n",
            "Epoch 26/100, Loss: 317.2012\n",
            "Epoch 27/100, Loss: 315.7739\n",
            "Epoch 28/100, Loss: 320.9980\n",
            "Epoch 29/100, Loss: 328.7326\n",
            "Epoch 30/100, Loss: 325.3058\n",
            "Epoch 31/100, Loss: 324.5121\n",
            "Epoch 32/100, Loss: 324.7326\n",
            "Epoch 33/100, Loss: 321.4706\n",
            "Epoch 34/100, Loss: 321.5275\n",
            "Epoch 35/100, Loss: 317.7685\n",
            "Epoch 36/100, Loss: 316.1562\n",
            "Epoch 37/100, Loss: 320.1298\n",
            "Epoch 38/100, Loss: 323.5685\n",
            "Epoch 39/100, Loss: 327.6963\n",
            "Epoch 40/100, Loss: 319.1199\n",
            "Epoch 41/100, Loss: 318.1408\n",
            "Epoch 42/100, Loss: 326.7525\n",
            "Epoch 43/100, Loss: 321.3838\n",
            "Epoch 44/100, Loss: 316.7977\n",
            "Epoch 45/100, Loss: 320.8226\n",
            "Epoch 46/100, Loss: 318.3522\n",
            "Epoch 47/100, Loss: 317.5230\n",
            "Epoch 48/100, Loss: 320.9687\n",
            "Epoch 49/100, Loss: 319.3468\n",
            "Epoch 50/100, Loss: 319.5014\n",
            "Epoch 51/100, Loss: 322.2808\n",
            "Epoch 52/100, Loss: 316.3729\n",
            "Epoch 53/100, Loss: 315.6908\n",
            "Epoch 54/100, Loss: 316.4686\n",
            "Epoch 55/100, Loss: 322.5742\n",
            "Epoch 56/100, Loss: 324.4968\n",
            "Epoch 57/100, Loss: 322.5184\n",
            "Epoch 58/100, Loss: 318.2795\n",
            "Epoch 59/100, Loss: 324.7737\n",
            "Epoch 60/100, Loss: 317.7459\n",
            "Epoch 61/100, Loss: 319.9074\n",
            "Epoch 62/100, Loss: 316.9419\n",
            "Epoch 63/100, Loss: 321.3239\n",
            "Epoch 64/100, Loss: 317.0422\n",
            "Epoch 65/100, Loss: 323.5376\n",
            "Epoch 66/100, Loss: 323.7937\n",
            "Epoch 67/100, Loss: 321.1807\n",
            "Epoch 68/100, Loss: 321.6260\n",
            "Epoch 69/100, Loss: 317.6772\n",
            "Epoch 70/100, Loss: 321.5412\n",
            "Epoch 71/100, Loss: 329.2689\n",
            "Epoch 72/100, Loss: 321.5664\n",
            "Epoch 73/100, Loss: 322.5916\n",
            "Epoch 74/100, Loss: 325.0402\n",
            "Epoch 75/100, Loss: 324.6961\n",
            "Epoch 76/100, Loss: 326.6870\n",
            "Epoch 77/100, Loss: 319.4173\n",
            "Epoch 78/100, Loss: 322.3935\n",
            "Epoch 79/100, Loss: 323.6993\n",
            "Epoch 80/100, Loss: 319.0678\n",
            "Epoch 81/100, Loss: 322.8677\n",
            "Epoch 82/100, Loss: 320.0667\n",
            "Epoch 83/100, Loss: 322.0437\n",
            "Epoch 84/100, Loss: 317.2382\n",
            "Epoch 85/100, Loss: 321.0476\n",
            "Epoch 86/100, Loss: 319.0759\n",
            "Epoch 87/100, Loss: 318.7551\n",
            "Epoch 88/100, Loss: 320.9031\n",
            "Epoch 89/100, Loss: 316.6260\n",
            "Epoch 90/100, Loss: 329.5476\n",
            "Epoch 91/100, Loss: 316.5448\n",
            "Epoch 92/100, Loss: 325.2810\n",
            "Epoch 93/100, Loss: 321.1346\n",
            "Epoch 94/100, Loss: 324.9641\n",
            "Epoch 95/100, Loss: 322.5700\n",
            "Epoch 96/100, Loss: 317.4758\n",
            "Epoch 97/100, Loss: 320.0134\n",
            "Epoch 98/100, Loss: 316.0124\n",
            "Epoch 99/100, Loss: 320.5689\n",
            "Epoch 100/100, Loss: 318.4669\n",
            "Predicted MaxCut value for the square graph: 5.52\n",
            "True MaxCut value for the square graph: 10.00\n",
            "Approximation Ratio: 0.55\n"
          ]
        }
      ]
    }
  ]
}