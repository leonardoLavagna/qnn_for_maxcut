{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## notebooks.LSTM_for_maxcut"
      ],
      "metadata": {
        "id": "AmqMgl3wENKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "ekXGJXr0zJWK"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = 6\n",
        "hidden_size = 16\n",
        "output_size = 1\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "gu_Pscq0zRnC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_dataset(num_samples=1000, graph_size=4, seed=42):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    datasets = []\n",
        "    labels = []\n",
        "    for _ in range(num_samples):\n",
        "        q = np.random.randint(-5, 6, (graph_size, graph_size))\n",
        "        q_s = (q + q.T) / 2\n",
        "        spins = np.random.choice([-1, 1], size=graph_size)\n",
        "        cut_value = 0\n",
        "        for i in range(graph_size):\n",
        "            for j in range(i + 1, graph_size):\n",
        "                if spins[i] != spins[j]:\n",
        "                    cut_value += abs(q_s[i, j])\n",
        "        input_features = q_s[np.triu_indices(graph_size, k=1)]\n",
        "        datasets.append(input_features)\n",
        "        labels.append(cut_value)\n",
        "    return torch.tensor(datasets, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "toxW4dymzMbW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def exact_maxcut(q_s):\n",
        "    n = q_s.shape[0]\n",
        "    max_cut = 0\n",
        "    for partition in range(1, 1 << n - 1):\n",
        "        set_a = [i for i in range(n) if (partition & (1 << i))]\n",
        "        set_b = [i for i in range(n) if not (partition & (1 << i))]\n",
        "        cut_value = 0\n",
        "        for i in set_a:\n",
        "            for j in set_b:\n",
        "                cut_value += abs(q_s[i, j])\n",
        "\n",
        "        max_cut = max(max_cut, cut_value)\n",
        "    return max_cut"
      ],
      "metadata": {
        "id": "q3L_Eg8I0UAQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MaxCutLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MaxCutLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "    def forward(self, x):\n",
        "        h_0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "        c_0 = torch.zeros(1, x.size(0), self.hidden_size)\n",
        "\n",
        "        out, _ = self.lstm(x, (h_0, c_0))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ],
      "metadata": {
        "id": "idyJG-uzzboq"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=100):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.unsqueeze(1))\n",
        "            loss = criterion(outputs.squeeze(), labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs.unsqueeze(1))\n",
        "                loss = criterion(outputs.squeeze(), labels)\n",
        "                val_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}\")\n"
      ],
      "metadata": {
        "id": "HjR0oIiA0HqR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = generate_dataset()\n",
        "val_data, val_labels = generate_dataset()\n",
        "train_loader = torch.utils.data.DataLoader(list(zip(train_data, train_labels)), batch_size=32, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(list(zip(val_data, val_labels)), batch_size=32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht3u99GGzdJM",
        "outputId": "dea7ab4f-dfd5-4549-dbbb-6e6a7f21dd36"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-af6b6dd00e09>:18: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:278.)\n",
            "  return torch.tensor(datasets, dtype=torch.float32), torch.tensor(labels, dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = MaxCutLSTM(input_size, hidden_size, output_size)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_model(model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_Bj_bI-0Dg8",
        "outputId": "3fd90f76-1fff-42f9-cd13-131529069a64"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 37.2645, Val Loss: 37.3201\n",
            "Epoch 2/100, Train Loss: 36.7320, Val Loss: 36.0411\n",
            "Epoch 3/100, Train Loss: 35.0300, Val Loss: 34.3865\n",
            "Epoch 4/100, Train Loss: 33.2940, Val Loss: 32.3340\n",
            "Epoch 5/100, Train Loss: 30.7711, Val Loss: 29.9270\n",
            "Epoch 6/100, Train Loss: 28.3507, Val Loss: 27.4104\n",
            "Epoch 7/100, Train Loss: 26.1980, Val Loss: 24.8046\n",
            "Epoch 8/100, Train Loss: 23.3840, Val Loss: 22.3508\n",
            "Epoch 9/100, Train Loss: 20.8424, Val Loss: 20.1067\n",
            "Epoch 10/100, Train Loss: 18.9226, Val Loss: 18.0772\n",
            "Epoch 11/100, Train Loss: 16.9116, Val Loss: 16.3255\n",
            "Epoch 12/100, Train Loss: 15.6379, Val Loss: 14.8224\n",
            "Epoch 13/100, Train Loss: 13.8918, Val Loss: 13.5830\n",
            "Epoch 14/100, Train Loss: 12.8752, Val Loss: 12.5502\n",
            "Epoch 15/100, Train Loss: 12.0518, Val Loss: 11.7423\n",
            "Epoch 16/100, Train Loss: 11.3218, Val Loss: 11.1198\n",
            "Epoch 17/100, Train Loss: 11.1517, Val Loss: 10.6599\n",
            "Epoch 18/100, Train Loss: 10.4970, Val Loss: 10.3036\n",
            "Epoch 19/100, Train Loss: 9.9796, Val Loss: 10.0507\n",
            "Epoch 20/100, Train Loss: 9.9643, Val Loss: 9.8751\n",
            "Epoch 21/100, Train Loss: 9.7485, Val Loss: 9.7469\n",
            "Epoch 22/100, Train Loss: 9.6496, Val Loss: 9.6676\n",
            "Epoch 23/100, Train Loss: 9.6996, Val Loss: 9.6027\n",
            "Epoch 24/100, Train Loss: 9.5130, Val Loss: 9.5599\n",
            "Epoch 25/100, Train Loss: 9.5242, Val Loss: 9.5160\n",
            "Epoch 26/100, Train Loss: 9.3374, Val Loss: 9.4883\n",
            "Epoch 27/100, Train Loss: 9.5108, Val Loss: 9.4637\n",
            "Epoch 28/100, Train Loss: 9.3878, Val Loss: 9.4413\n",
            "Epoch 29/100, Train Loss: 9.5606, Val Loss: 9.4213\n",
            "Epoch 30/100, Train Loss: 9.4111, Val Loss: 9.4053\n",
            "Epoch 31/100, Train Loss: 9.3963, Val Loss: 9.3846\n",
            "Epoch 32/100, Train Loss: 9.3268, Val Loss: 9.3665\n",
            "Epoch 33/100, Train Loss: 9.3411, Val Loss: 9.3484\n",
            "Epoch 34/100, Train Loss: 9.5227, Val Loss: 9.3271\n",
            "Epoch 35/100, Train Loss: 9.4655, Val Loss: 9.3069\n",
            "Epoch 36/100, Train Loss: 9.2786, Val Loss: 9.2876\n",
            "Epoch 37/100, Train Loss: 9.1216, Val Loss: 9.2671\n",
            "Epoch 38/100, Train Loss: 9.2642, Val Loss: 9.2477\n",
            "Epoch 39/100, Train Loss: 9.2921, Val Loss: 9.2302\n",
            "Epoch 40/100, Train Loss: 9.2644, Val Loss: 9.2100\n",
            "Epoch 41/100, Train Loss: 9.2431, Val Loss: 9.1898\n",
            "Epoch 42/100, Train Loss: 9.1646, Val Loss: 9.1708\n",
            "Epoch 43/100, Train Loss: 9.1285, Val Loss: 9.1504\n",
            "Epoch 44/100, Train Loss: 9.1343, Val Loss: 9.1285\n",
            "Epoch 45/100, Train Loss: 9.3025, Val Loss: 9.1093\n",
            "Epoch 46/100, Train Loss: 9.1986, Val Loss: 9.0853\n",
            "Epoch 47/100, Train Loss: 8.9971, Val Loss: 9.0651\n",
            "Epoch 48/100, Train Loss: 8.9788, Val Loss: 9.0454\n",
            "Epoch 49/100, Train Loss: 8.8639, Val Loss: 9.0225\n",
            "Epoch 50/100, Train Loss: 9.1751, Val Loss: 9.0030\n",
            "Epoch 51/100, Train Loss: 8.9803, Val Loss: 8.9814\n",
            "Epoch 52/100, Train Loss: 8.9022, Val Loss: 8.9591\n",
            "Epoch 53/100, Train Loss: 8.9422, Val Loss: 8.9378\n",
            "Epoch 54/100, Train Loss: 8.9497, Val Loss: 8.9139\n",
            "Epoch 55/100, Train Loss: 8.9479, Val Loss: 8.8916\n",
            "Epoch 56/100, Train Loss: 8.7606, Val Loss: 8.8733\n",
            "Epoch 57/100, Train Loss: 8.7712, Val Loss: 8.8531\n",
            "Epoch 58/100, Train Loss: 8.7814, Val Loss: 8.8309\n",
            "Epoch 59/100, Train Loss: 8.7644, Val Loss: 8.8093\n",
            "Epoch 60/100, Train Loss: 8.8100, Val Loss: 8.7872\n",
            "Epoch 61/100, Train Loss: 8.7336, Val Loss: 8.7650\n",
            "Epoch 62/100, Train Loss: 8.8214, Val Loss: 8.7475\n",
            "Epoch 63/100, Train Loss: 8.7701, Val Loss: 8.7264\n",
            "Epoch 64/100, Train Loss: 8.5640, Val Loss: 8.7017\n",
            "Epoch 65/100, Train Loss: 8.6429, Val Loss: 8.6768\n",
            "Epoch 66/100, Train Loss: 8.5604, Val Loss: 8.6535\n",
            "Epoch 67/100, Train Loss: 8.9559, Val Loss: 8.6311\n",
            "Epoch 68/100, Train Loss: 8.5270, Val Loss: 8.6086\n",
            "Epoch 69/100, Train Loss: 8.5301, Val Loss: 8.5851\n",
            "Epoch 70/100, Train Loss: 8.7224, Val Loss: 8.5617\n",
            "Epoch 71/100, Train Loss: 8.5766, Val Loss: 8.5374\n",
            "Epoch 72/100, Train Loss: 8.4870, Val Loss: 8.5165\n",
            "Epoch 73/100, Train Loss: 8.4923, Val Loss: 8.4961\n",
            "Epoch 74/100, Train Loss: 8.4398, Val Loss: 8.4692\n",
            "Epoch 75/100, Train Loss: 8.3482, Val Loss: 8.4454\n",
            "Epoch 76/100, Train Loss: 8.3355, Val Loss: 8.4181\n",
            "Epoch 77/100, Train Loss: 8.5032, Val Loss: 8.3905\n",
            "Epoch 78/100, Train Loss: 8.3809, Val Loss: 8.3683\n",
            "Epoch 79/100, Train Loss: 8.2952, Val Loss: 8.3444\n",
            "Epoch 80/100, Train Loss: 8.3674, Val Loss: 8.3192\n",
            "Epoch 81/100, Train Loss: 8.3012, Val Loss: 8.2919\n",
            "Epoch 82/100, Train Loss: 8.2687, Val Loss: 8.2672\n",
            "Epoch 83/100, Train Loss: 8.2431, Val Loss: 8.2428\n",
            "Epoch 84/100, Train Loss: 8.1635, Val Loss: 8.2166\n",
            "Epoch 85/100, Train Loss: 8.1660, Val Loss: 8.1905\n",
            "Epoch 86/100, Train Loss: 8.2259, Val Loss: 8.1686\n",
            "Epoch 87/100, Train Loss: 8.2231, Val Loss: 8.1404\n",
            "Epoch 88/100, Train Loss: 8.1688, Val Loss: 8.1125\n",
            "Epoch 89/100, Train Loss: 8.2299, Val Loss: 8.0858\n",
            "Epoch 90/100, Train Loss: 8.1693, Val Loss: 8.0642\n",
            "Epoch 91/100, Train Loss: 8.1578, Val Loss: 8.0397\n",
            "Epoch 92/100, Train Loss: 8.1249, Val Loss: 8.0127\n",
            "Epoch 93/100, Train Loss: 7.9728, Val Loss: 7.9904\n",
            "Epoch 94/100, Train Loss: 7.9449, Val Loss: 7.9654\n",
            "Epoch 95/100, Train Loss: 8.0013, Val Loss: 7.9395\n",
            "Epoch 96/100, Train Loss: 8.1382, Val Loss: 7.9149\n",
            "Epoch 97/100, Train Loss: 7.7978, Val Loss: 7.8895\n",
            "Epoch 98/100, Train Loss: 7.9087, Val Loss: 7.8677\n",
            "Epoch 99/100, Train Loss: 7.8396, Val Loss: 7.8427\n",
            "Epoch 100/100, Train Loss: 7.7599, Val Loss: 7.8189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q_s = np.array([[0, 3, 0, -2],\n",
        "                [3, 0, 4, 0],\n",
        "                [0, 4, 0, 1],\n",
        "                [-2, 0, 1, 0]])\n",
        "input_features = q_s[np.triu_indices(4, k=1)]\n",
        "test_input = torch.tensor(input_features, dtype=torch.float32).unsqueeze(0)"
      ],
      "metadata": {
        "id": "HATWS69dzH-C"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predicted_cut = model(test_input.unsqueeze(1)).item()\n",
        "true_maxcut = exact_maxcut(q_s)\n",
        "approximation_ratio = predicted_cut / true_maxcut if true_maxcut != 0 else 0\n",
        "print(f\"Predicted MaxCut value for the square graph: {predicted_cut:.2f}\")\n",
        "print(f\"True MaxCut value for the square graph: {true_maxcut:.2f}\")\n",
        "print(f\"Approximation Ratio: {approximation_ratio:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeZretebnZ-Z",
        "outputId": "4f7f8706-514e-4815-b9d5-351dbce88bb9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted MaxCut value for the square graph: 4.90\n",
            "True MaxCut value for the square graph: 10.00\n",
            "Approximation Ratio: 0.49\n"
          ]
        }
      ]
    }
  ]
}